{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTnpA365X7r9"
      },
      "outputs": [],
      "source": [
        "# Import standard libraries for randomness, deep copying, and numerical operations\n",
        "import random\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Import libraries for image processing and data manipulation\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# Import PyTorch core and utilities for deep learning\n",
        "import torch\n",
        "import torch.optim as optim  # Optimization algorithms\n",
        "import torch.nn as nn  # Neural network modules\n",
        "import torch.nn.functional as F  # Functional API for non-parametric operations\n",
        "\n",
        "# Import PyTorch utilities for data loading and transformations\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler\n",
        "from torchvision.transforms.v2 import Compose, ToImage, Normalize, ToPILImage, Resize, ToDtype\n",
        "from torchvision import transforms\n",
        "\n",
        "# Import dataset handling and learning rate schedulers\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, MultiStepLR, CyclicLR, LambdaLR\n",
        "\n",
        "# Import visualization and web utilities\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import errno\n",
        "\n",
        "# Set matplotlib style for better visuals\n",
        "#plt.style.use('fivethirtyeight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h38OikK_-d4M"
      },
      "source": [
        "## Architecture class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYxCEnws8T7L"
      },
      "outputs": [],
      "source": [
        "class Architecture(object):\n",
        "    def __init__(self, model, loss_fn, optimizer):\n",
        "        # Here we define the attributes of our class\n",
        "\n",
        "        # We start by storing the arguments as attributes\n",
        "        # to use them later\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        \n",
        "        # Let's send the model to the specified device right away\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # These attributes are defined here, but since they are\n",
        "        # not informed at the moment of creation, we keep them None\n",
        "        self.train_loader = None\n",
        "        self.val_loader = None\n",
        "\n",
        "        # These attributes are going to be computed internally\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.total_epochs = 0\n",
        "\n",
        "        # Creates the train_step function for our model,\n",
        "        # loss function and optimizer\n",
        "        # Note: there are NO ARGS there! It makes use of the class\n",
        "        # attributes directly\n",
        "        self.train_step_fn = self._make_train_step_fn()\n",
        "        # Creates the val_step function for our model and loss\n",
        "        self.val_step_fn = self._make_val_step_fn()\n",
        "\n",
        "        # for hook purposes\n",
        "        self.handles = {}\n",
        "        self.visualization = {}\n",
        "\n",
        "    def to(self, device):\n",
        "        # This method allows the user to specify a different device\n",
        "        # It sets the corresponding attribute (to be used later in\n",
        "        # the mini-batches) and sends the model to the device\n",
        "        try:\n",
        "            self.device = device\n",
        "            self.model.to(self.device)\n",
        "        except RuntimeError:\n",
        "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            print(f\"Couldn't send it to {device}, sending it to {self.device} instead.\")\n",
        "            self.model.to(self.device)\n",
        "\n",
        "    def set_loaders(self, train_loader, val_loader=None):\n",
        "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
        "        # Both loaders are then assigned to attributes of the class\n",
        "        # So they can be referred to later\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "    def _make_train_step_fn(self):\n",
        "        # This method does not need ARGS... it can refer to\n",
        "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
        "\n",
        "        # Builds function that performs a step in the train loop\n",
        "        def perform_train_step_fn(x, y):\n",
        "            # Sets model to TRAIN mode\n",
        "            self.model.train()\n",
        "\n",
        "            # Step 1 - Computes our model's predicted output - forward pass\n",
        "            yhat = self.model(x)\n",
        "            # Step 2 - Computes the loss\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
        "            loss.backward()\n",
        "            # Step 4 - Updates parameters using gradients and the learning rate\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Returns the loss\n",
        "            return loss.item()\n",
        "\n",
        "        # Returns the function that will be called inside the train loop\n",
        "        return perform_train_step_fn\n",
        "\n",
        "    def _make_val_step_fn(self):\n",
        "        # Builds function that performs a step in the validation loop\n",
        "        def perform_val_step_fn(x, y):\n",
        "            # Sets model to EVAL mode\n",
        "            self.model.eval()\n",
        "\n",
        "            # Step 1 - Computes our model's predicted output - forward pass\n",
        "            yhat = self.model(x)\n",
        "            # Step 2 - Computes the loss\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n",
        "            return loss.item()\n",
        "\n",
        "        return perform_val_step_fn\n",
        "\n",
        "    def _mini_batch(self, validation=False):\n",
        "        # The mini-batch can be used with both loaders\n",
        "        # The argument `validation`defines which loader and\n",
        "        # corresponding step function is going to be used\n",
        "        if validation:\n",
        "            data_loader = self.val_loader\n",
        "            step_fn = self.val_step_fn\n",
        "        else:\n",
        "            data_loader = self.train_loader\n",
        "            step_fn = self.train_step_fn\n",
        "\n",
        "        if data_loader is None:\n",
        "            return None\n",
        "\n",
        "        # Once the data loader and step function, this is the same\n",
        "        # mini-batch loop we had before\n",
        "        mini_batch_losses = []\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch = x_batch.to(self.device)\n",
        "            y_batch = y_batch.to(self.device)\n",
        "\n",
        "            mini_batch_loss = step_fn(x_batch, y_batch)\n",
        "            mini_batch_losses.append(mini_batch_loss)\n",
        "\n",
        "        loss = np.mean(mini_batch_losses)\n",
        "        return loss\n",
        "\n",
        "    # this function was updated in this class\n",
        "    def set_seed(self, seed=42):\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "        try:\n",
        "            self.train_loader.sampler.generator.manual_seed(seed)\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "    def train(self, n_epochs, seed=42):\n",
        "        # To ensure reproducibility of the training process\n",
        "        self.set_seed(seed)\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            # Keeps track of the numbers of epochs\n",
        "            # by updating the corresponding attribute\n",
        "            self.total_epochs += 1\n",
        "\n",
        "            # inner loop\n",
        "            # Performs training using mini-batches\n",
        "            loss = self._mini_batch(validation=False)\n",
        "            self.losses.append(loss)\n",
        "\n",
        "            # VALIDATION\n",
        "            # no gradients in validation!\n",
        "            with torch.no_grad():\n",
        "                # Performs evaluation using mini-batches\n",
        "                val_loss = self._mini_batch(validation=True)\n",
        "                self.val_losses.append(val_loss)\n",
        "\n",
        "    def save_checkpoint(self, filename):\n",
        "        # Builds dictionary with all elements for resuming training\n",
        "        checkpoint = {'epoch': self.total_epochs,\n",
        "                      'model_state_dict': self.model.state_dict(),\n",
        "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                      'loss': self.losses,\n",
        "                      'val_loss': self.val_losses}\n",
        "\n",
        "        torch.save(checkpoint, filename)\n",
        "\n",
        "    def load_checkpoint(self, filename):\n",
        "        # Loads dictionary\n",
        "        checkpoint = torch.load(filename)\n",
        "\n",
        "        # Restore state for model and optimizer\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        self.total_epochs = checkpoint['epoch']\n",
        "        self.losses = checkpoint['loss']\n",
        "        self.val_losses = checkpoint['val_loss']\n",
        "\n",
        "        self.model.train() # always use TRAIN for resuming training\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Set is to evaluation mode for predictions\n",
        "        self.model.eval()\n",
        "        # Takes aNumpy input and make it a float tensor\n",
        "        x_tensor = torch.as_tensor(x).float()\n",
        "        # Send input to device and uses model for prediction\n",
        "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
        "        # Set it back to train mode\n",
        "        self.model.train()\n",
        "        # Detaches it, brings it to CPU and back to Numpy\n",
        "        return y_hat_tensor.detach().cpu().numpy()\n",
        "\n",
        "    def count_parameters(self):\n",
        "      return sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "\n",
        "    def plot_losses(self):\n",
        "        fig = plt.figure(figsize=(10, 4))\n",
        "        plt.plot(self.losses, label='Training Loss', c='g')\n",
        "        plt.plot(self.val_losses, label='Validation Loss', c='y')\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    @staticmethod\n",
        "    def _visualize_tensors(axs, x, y=None, yhat=None, layer_name='', title=None):\n",
        "        # The number of images is the number of subplots in a row\n",
        "        n_images = len(axs)\n",
        "        # Gets max and min values for scaling the grayscale\n",
        "        minv, maxv = np.min(x[:n_images]), np.max(x[:n_images])\n",
        "        # For each image\n",
        "        for j, image in enumerate(x[:n_images]):\n",
        "            ax = axs[j]\n",
        "            # Sets title, labels, and removes ticks\n",
        "            if title is not None:\n",
        "                ax.set_title(f'{title} #{j}', fontsize=12)\n",
        "            shp = np.atleast_2d(image).shape\n",
        "            ax.set_ylabel(\n",
        "                f'{layer_name}\\n{shp[0]}x{shp[1]}',\n",
        "                rotation=0, labelpad=40\n",
        "            )\n",
        "            xlabel1 = '' if y is None else f'\\nLabel: {y[j]}'\n",
        "            xlabel2 = '' if yhat is None else f'\\nPredicted: {yhat[j]}'\n",
        "            xlabel = f'{xlabel1}{xlabel2}'\n",
        "            if len(xlabel):\n",
        "                ax.set_xlabel(xlabel, fontsize=12)\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "\n",
        "            # Plots weight as an image\n",
        "            ax.imshow(\n",
        "                np.atleast_2d(image.squeeze()),\n",
        "                cmap='gray',\n",
        "                vmin=minv,\n",
        "                vmax=maxv\n",
        "            )\n",
        "        return\n",
        "\n",
        "    def visualize_filters(self, layer_name, **kwargs):\n",
        "        try:\n",
        "            # Gets the layer object from the model\n",
        "            layer = self.model\n",
        "            for name in layer_name.split('.'):\n",
        "                layer = getattr(layer, name)\n",
        "            # We are only looking at filters for 2D convolutions\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                # Takes the weight information\n",
        "                weights = layer.weight.data.cpu().numpy()\n",
        "                # weights -> (channels_out (filter), channels_in, H, W)\n",
        "                n_filters, n_channels, _, _ = weights.shape\n",
        "\n",
        "                # Builds a figure\n",
        "                size = (2 * n_channels + 2, 2 * n_filters)\n",
        "                fig, axes = plt.subplots(n_filters, n_channels,\n",
        "                                        figsize=size)\n",
        "                axes = np.atleast_2d(axes)\n",
        "                axes = axes.reshape(n_filters, n_channels)\n",
        "                # For each channel_out (filter)\n",
        "                for i in range(n_filters):\n",
        "                    Architecture._visualize_tensors(\n",
        "                        axes[i, :],\n",
        "                        weights[i],\n",
        "                        layer_name=f'Filter #{i}',\n",
        "                        title='Channel'\n",
        "                    )\n",
        "\n",
        "                for ax in axes.flat:\n",
        "                    ax.label_outer()\n",
        "\n",
        "                fig.tight_layout()\n",
        "                return fig\n",
        "        except AttributeError:\n",
        "            return\n",
        "\n",
        "    def attach_hooks(self, layers_to_hook, hook_fn=None):\n",
        "        # Clear any previous values\n",
        "        self.visualization = {}\n",
        "        # Creates the dictionary to map layer objects to their names\n",
        "        modules = list(self.model.named_modules())\n",
        "        layer_names = {layer: name for name, layer in modules[1:]}\n",
        "\n",
        "        if hook_fn is None:\n",
        "            # Hook function to be attached to the forward pass\n",
        "            def hook_fn(layer, inputs, outputs):\n",
        "                # Gets the layer name\n",
        "                name = layer_names[layer]\n",
        "                # Detaches outputs\n",
        "                values = outputs.detach().cpu().numpy()\n",
        "                # Since the hook function may be called multiple times\n",
        "                # for example, if we make predictions for multiple mini-batches\n",
        "                # it concatenates the results\n",
        "                if self.visualization[name] is None:\n",
        "                    self.visualization[name] = values\n",
        "                else:\n",
        "                    self.visualization[name] = np.concatenate([self.visualization[name], values])\n",
        "\n",
        "        for name, layer in modules:\n",
        "            # If the layer is in our list\n",
        "            if name in layers_to_hook:\n",
        "                # Initializes the corresponding key in the dictionary\n",
        "                self.visualization[name] = None\n",
        "                # Register the forward hook and keep the handle in another dict\n",
        "                self.handles[name] = layer.register_forward_hook(hook_fn)\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        # Loops through all hooks and removes them\n",
        "        for handle in self.handles.values():\n",
        "            handle.remove()\n",
        "        # Clear the dict, as all hooks have been removed\n",
        "        self.handles = {}\n",
        "\n",
        "    def visualize_outputs(self, layers, n_images=10, y=None, yhat=None):\n",
        "        layers = filter(lambda l: l in self.visualization.keys(), layers)\n",
        "        layers = list(layers)\n",
        "        shapes = [self.visualization[layer].shape for layer in layers]\n",
        "        n_rows = [shape[1] if len(shape) == 4 else 1\n",
        "                  for shape in shapes]\n",
        "        total_rows = np.sum(n_rows)\n",
        "\n",
        "        fig, axes = plt.subplots(total_rows, n_images,\n",
        "                                figsize=(1.5*n_images, 1.5*total_rows))\n",
        "        axes = np.atleast_2d(axes).reshape(total_rows, n_images)\n",
        "\n",
        "        # Loops through the layers, one layer per row of subplots\n",
        "        row = 0\n",
        "        for i, layer in enumerate(layers):\n",
        "            start_row = row\n",
        "            # Takes the produced feature maps for that layer\n",
        "            output = self.visualization[layer]\n",
        "\n",
        "            is_vector = len(output.shape) == 2\n",
        "\n",
        "            for j in range(n_rows[i]):\n",
        "                Architecture._visualize_tensors(\n",
        "                    axes[row, :],\n",
        "                    output if is_vector else output[:, j].squeeze(),\n",
        "                    y,\n",
        "                    yhat,\n",
        "                    layer_name=layers[i] \\\n",
        "                              if is_vector \\\n",
        "                              else f'{layers[i]}\\nfil#{row-start_row}',\n",
        "                    title='Image' if (row == 0) else None\n",
        "                )\n",
        "                row += 1\n",
        "\n",
        "        for ax in axes.flat:\n",
        "            ax.label_outer()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def correct(self, x, y, threshold=.5):\n",
        "        self.model.eval()\n",
        "        yhat = self.model(x.to(self.device))\n",
        "        y = y.to(self.device)\n",
        "        self.model.train()\n",
        "\n",
        "        # We get the size of the batch and the number of classes\n",
        "        # (only 1, if it is binary)\n",
        "        n_samples, n_dims = yhat.shape\n",
        "        if n_dims > 1:\n",
        "            # In a multiclass classification, the biggest logit\n",
        "            # always wins, so we don't bother getting probabilities\n",
        "\n",
        "            # This is PyTorch's version of argmax,\n",
        "            # but it returns a tuple: (max value, index of max value)\n",
        "            _, predicted = torch.max(yhat, 1)\n",
        "        else:\n",
        "            n_dims += 1\n",
        "            # In binary classification, we NEED to check if the\n",
        "            # last layer is a sigmoid (and then it produces probs)\n",
        "            if isinstance(self.model, nn.Sequential) and \\\n",
        "              isinstance(self.model[-1], nn.Sigmoid):\n",
        "                predicted = (yhat > threshold).long()\n",
        "            # or something else (logits), which we need to convert\n",
        "            # using a sigmoid\n",
        "            else:\n",
        "                predicted = (F.sigmoid(yhat) > threshold).long()\n",
        "\n",
        "        # How many samples got classified correctly for each class\n",
        "        result = []\n",
        "        for c in range(n_dims):\n",
        "            n_class = (y == c).sum().item()\n",
        "            n_correct = (predicted[y == c] == c).sum().item()\n",
        "            result.append((n_correct, n_class))\n",
        "        return torch.tensor(result)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def loader_apply(loader, func, reduce='sum'):\n",
        "        results = [func(x, y) for i, (x, y) in enumerate(loader)]\n",
        "        results = torch.stack(results, axis=0)\n",
        "\n",
        "        if reduce == 'sum':\n",
        "            results = results.sum(axis=0)\n",
        "        elif reduce == 'mean':\n",
        "            results = results.float().mean(axis=0)\n",
        "\n",
        "        return results\n",
        "\n",
        "    @staticmethod\n",
        "    def statistics_per_channel(images, labels):\n",
        "        # NCHW\n",
        "        n_samples, n_channels, n_height, n_weight = images.size()\n",
        "        # Flatten HW into a single dimension\n",
        "        flatten_per_channel = images.reshape(n_samples, n_channels, -1)\n",
        "\n",
        "        # Computes statistics of each image per channel\n",
        "        # Average pixel value per channel\n",
        "        # (n_samples, n_channels)\n",
        "        means = flatten_per_channel.mean(axis=2)\n",
        "        # Standard deviation of pixel values per channel\n",
        "        # (n_samples, n_channels)\n",
        "        stds = flatten_per_channel.std(axis=2)\n",
        "\n",
        "        # Adds up statistics of all images in a mini-batch\n",
        "        # (1, n_channels)\n",
        "        sum_means = means.sum(axis=0)\n",
        "        sum_stds = stds.sum(axis=0)\n",
        "        # Makes a tensor of shape (1, n_channels)\n",
        "        # with the number of samples in the mini-batch\n",
        "        n_samples = torch.tensor([n_samples]*n_channels).float()\n",
        "\n",
        "        # Stack the three tensors on top of one another\n",
        "        # (3, n_channels)\n",
        "        return torch.stack([n_samples, sum_means, sum_stds], axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def make_normalizer(loader):\n",
        "        total_samples, total_means, total_stds = Architecture.loader_apply(loader, Architecture.statistics_per_channel)\n",
        "        norm_mean = total_means / total_samples\n",
        "        norm_std = total_stds / total_samples\n",
        "        return Normalize(mean=norm_mean, std=norm_std)\n",
        "\n",
        "    def lr_range_test(self, data_loader, end_lr, num_iter=100, step_mode='exp', alpha=0.05, ax=None):\n",
        "        # Since the test updates both model and optimizer we need to store\n",
        "        # their initial states to restore them in the end\n",
        "        previous_states = {'model': deepcopy(self.model.state_dict()),\n",
        "                          'optimizer': deepcopy(self.optimizer.state_dict())}\n",
        "        # Retrieves the learning rate set in the optimizer\n",
        "        start_lr = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
        "\n",
        "        # Builds a custom function and corresponding scheduler\n",
        "        lr_fn = make_lr_fn(start_lr, end_lr, num_iter)\n",
        "        scheduler = LambdaLR(self.optimizer, lr_lambda=lr_fn)    \n",
        "\n",
        "        # Variables for tracking results and iterations\n",
        "        tracking = {'loss': [], 'lr': []}\n",
        "        iteration = 0\n",
        "\n",
        "        # If there are more iterations than mini-batches in the data loader,\n",
        "        # it will have to loop over it more than once\n",
        "        while (iteration < num_iter):\n",
        "            # That's the typical mini-batch inner loop\n",
        "            for x_batch, y_batch in data_loader:\n",
        "                x_batch = x_batch.to(self.device)\n",
        "                y_batch = y_batch.to(self.device)\n",
        "                # Step 1\n",
        "                yhat = self.model(x_batch)\n",
        "                # Step 2\n",
        "                loss = self.loss_fn(yhat, y_batch)\n",
        "                # Step 3\n",
        "                loss.backward()\n",
        "\n",
        "                # Here we keep track of the losses (smoothed)\n",
        "                # and the learning rates\n",
        "                tracking['lr'].append(scheduler.get_last_lr()[0])\n",
        "                if iteration == 0:\n",
        "                    tracking['loss'].append(loss.item())\n",
        "                else:\n",
        "                    prev_loss = tracking['loss'][-1]\n",
        "                    smoothed_loss = alpha * loss.item() + (1-alpha) * prev_loss\n",
        "                    tracking['loss'].append(smoothed_loss)\n",
        "\n",
        "                iteration += 1\n",
        "                # Number of iterations reached\n",
        "                if iteration == num_iter:\n",
        "                    break\n",
        "\n",
        "                # Step 4\n",
        "                self.optimizer.step()\n",
        "                scheduler.step()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "        # Restores the original states\n",
        "        self.optimizer.load_state_dict(previous_states['optimizer'])\n",
        "        self.model.load_state_dict(previous_states['model'])\n",
        "\n",
        "        if ax is None:\n",
        "            fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
        "        else:\n",
        "            fig = ax.get_figure()\n",
        "        ax.plot(tracking['lr'], tracking['loss'])\n",
        "        if step_mode == 'exp':\n",
        "            ax.set_xscale('log')\n",
        "        ax.set_xlabel('Learning Rate')\n",
        "        ax.set_ylabel('Loss')\n",
        "        fig.tight_layout()\n",
        "        return tracking, fig\n",
        "\n",
        "    def set_optimizer(self, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ht9VCubz7DU"
      },
      "source": [
        "# Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14goKgXj0CwA"
      },
      "source": [
        ">This dataset was created by combining two datasets from Kaggle: [Thitinan Kliangsuwan. Cloud Type Classification 3, 2022.](https://kaggle.com/competitions/cloud-type-classification-3) and [Howard-Cloud-X](https://www.kaggle.com/datasets/imbikramsaha/howard-cloudx). \n",
        "\n",
        "\n",
        "\n",
        ">The Howard-Cloud-X is licensed as [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to the dataset\n",
        "folder='../dataset/clouds_types'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxA6hZl_ys8D"
      },
      "outputs": [],
      "source": [
        "def figure1(folder):\n",
        "    #cirroculumulus = Image.open(f'{folder}/Cirroculumulus/cirroculumulus-002.jpg')\n",
        "    #cirrostratus = Image.open(f'{folder}/Cirrostratus/cirrostratus-024.jpg')\n",
        "    cirrus = Image.open(f'{folder}/Cirrus/cirrus-004.jpg')\n",
        "    altocumulus = Image.open(f'{folder}/Altocumulus/altocumulus-000.jpg')\n",
        "    #altostratus = Image.open(f'{folder}/Altostratus/altostratus-017.jpg')\n",
        "    cumulonimbus = Image.open(f'{folder}/Cumulonimbus/cumulonimbus-030.jpg')\n",
        "    cumulus = Image.open(f'{folder}/Cumulus/cumulus-000.jpg')\n",
        "    nimbostratus = Image.open(f'{folder}/Nimbostratus/nimbostratus-000.jpg')\n",
        "    clear_sky = Image.open(f'{folder}/Clear_Sky/clear_sky-008.jpg')\n",
        "    #stratus = Image.open(f'{folder}/Stratus/stratus-000.jpg')\n",
        "\n",
        "\n",
        "    images = [cirrus, altocumulus,cumulonimbus, cumulus, nimbostratus,clear_sky]\n",
        "    titles = ['Cirrus','Altocumulus','Cumulonimbus', 'Cumulus', 'Nimbostratus',\"Clear Sky\"]\n",
        "\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(12, 5))\n",
        "    for ax, image, title in zip(axs.flat, images, titles):\n",
        "        ax.imshow(image)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_title(title)\n",
        "\n",
        "    return fig\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "_SRQZfCay1fs",
        "outputId": "ad2bc82d-f856-404d-8539-895658347fdb"
      },
      "outputs": [],
      "source": [
        "fig = figure1(folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9wh98Ryx7Pd"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPpGYgTYx7Pe"
      },
      "source": [
        "## ImageFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usfX4Pymx7Pe"
      },
      "outputs": [],
      "source": [
        "# Compose a sequence of preprocessing transforms\n",
        "# 1) Resize images to 28×28 pixels\n",
        "# 2) Ensure output is a PIL/torchvision Image (dropping any alpha channel)\n",
        "# 3) Convert pixel values to float32 and scale from [0–255] to [0.0–1.0]\n",
        "\n",
        "temp_transform = transforms.Compose([\n",
        "    transforms.Resize((28,28)),        # Resize each image to 28×28\n",
        "    ToImage(),                         # Convert tensor back to PIL Image (enforces RGB)\n",
        "    ToDtype(torch.float32, scale=True) # Cast to float32 and normalize pixel range\n",
        "])\n",
        "\n",
        "# Create an ImageFolder dataset from the 'Cloud types' directory\n",
        "# Images are grouped by subfolder name as class labels, and each image is transformed\n",
        "temp_dataset = ImageFolder(\n",
        "    root='../dataset/clouds_types',  # Path to the dataset\n",
        "    transform=temp_transform      # Apply the preprocessing pipeline to every image\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGK-y5KAKymI",
        "outputId": "1bd8589f-4156-49e7-9d32-c6e1290ee9a7"
      },
      "outputs": [],
      "source": [
        "# Looking the shape of every images\n",
        "temp_dataset[len(temp_dataset)-1][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye2yR6BLAxys",
        "outputId": "1c2a615f-f55d-42c0-b3ed-1504cdfc55fa"
      },
      "outputs": [],
      "source": [
        "# Get total number of samples in the dataset\n",
        "dataset_size = len(temp_dataset)\n",
        "print(f\"Dataset size: {dataset_size} images\")\n",
        "\n",
        "# Get number of classes\n",
        "num_classes = len(temp_dataset.classes)\n",
        "print(f\"Number of classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnqFv6gyx7Pe"
      },
      "source": [
        "## Standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgMwU7Spx7Pe"
      },
      "outputs": [],
      "source": [
        "temp_loader = DataLoader(temp_dataset, batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-Eav2U_x7Pe",
        "outputId": "c31062bf-e71b-48bf-bd98-c044309495cc"
      },
      "outputs": [],
      "source": [
        "# Each column represents a channel\n",
        "# first row is the number of data points\n",
        "# second row is the the sum of mean values\n",
        "# third row is the sum of standard deviations\n",
        "first_images, first_labels = next(iter(temp_loader))\n",
        "Architecture.statistics_per_channel(first_images, first_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXu6vXn8x7Pf",
        "outputId": "40dc78b2-4483-4c4b-a88a-1749c1a8634c"
      },
      "outputs": [],
      "source": [
        "# We can leverage the loader_apply() method to get the sums for the whole dataset:\n",
        "results = Architecture.loader_apply(temp_loader, Architecture.statistics_per_channel)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWfYfTQZx7Pf",
        "outputId": "8d243385-a464-4abd-9228-b7840106be5f"
      },
      "outputs": [],
      "source": [
        "# we can compute the average mean value and the average standard deviation, per channel.\n",
        "# Better yet, let’s make it a method that takes a data loader and\n",
        "# returns an instance of the Normalize() transform\n",
        "normalizer = Architecture.make_normalizer(temp_loader)\n",
        "normalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OewfD2eLx7Pf"
      },
      "source": [
        "## The Real Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy5Gz8sgx7Pf"
      },
      "outputs": [],
      "source": [
        "# Define a pipeline of image transformations:\n",
        "# 1) Resize each image to 28×28 pixels\n",
        "# 2) Ensure the output is a PIL/torchvision image (dropping any alpha channel)\n",
        "# 3) Cast pixels to float32 and scale from [0–255] to [0.0–1.0]\n",
        "# 4) Apply the user-defined normalization (e.g., mean/std normalization)\n",
        "composer = transforms.Compose([\n",
        "    transforms.Resize((28,28)),         # Resize to 28×28\n",
        "    ToImage(),                          # Convert to PIL Image in RGB\n",
        "    ToDtype(torch.float32, scale=True), # Cast to float32 and normalize to [0,1]\n",
        "    normalizer                          # Apply custom normalization transform\n",
        "])\n",
        "\n",
        "# Instantiate training and validation datasets from folders:\n",
        "# - 'clouds_types' contains subfolders per class for training\n",
        "# - 'clouds_types_test' likewise for validation\n",
        "train_data = ImageFolder(root='../dataset/clouds_types', transform=composer)\n",
        "val_data   = ImageFolder(root='../dataset/clouds_types_test', transform=composer)\n",
        "\n",
        "# Wrap datasets in DataLoaders for batching and shuffling:\n",
        "# - batch_size=16 yields mini-batches of 16 images\n",
        "# - shuffle=True randomizes training order each epoch\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "val_loader   = DataLoader(val_data,   batch_size=16)  # no shuffle for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMKSojnWycbe"
      },
      "outputs": [],
      "source": [
        "# Function to get one sample per class from the dataset\n",
        "\n",
        "def get_one_per_class(dataset, num_classes):\n",
        "    samples = defaultdict(list)\n",
        "    for img, label in dataset:\n",
        "        if label not in samples:\n",
        "            samples[label] = img\n",
        "        if len(samples) == num_classes:\n",
        "            break\n",
        "    images = [samples[i] for i in range(num_classes)]\n",
        "    labels = list(range(num_classes))\n",
        "    return images, labels\n",
        "\n",
        "# Creating the plot\n",
        "\n",
        "def figure2(first_images, first_labels):\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(12, 4))\n",
        "    titles = ['Cirrus','Altocumulus','Cumulonimbus', 'Cumulus', 'Nimbostratus',\"Clear Sky\"]\n",
        "    \n",
        "    axs = axs.flatten()  # Flatten the 2D array of axes into a 1D array for easier indexing\n",
        "    \n",
        "    for i in range(6):\n",
        "        # Ensure first_images[i] is a PyTorch tensor before using ToPILImage\n",
        "        image_tensor = torch.tensor(first_images[i]) if isinstance(first_images[i], np.ndarray) else first_images[i]\n",
        "        image, label = ToPILImage()(image_tensor), first_labels[i]\n",
        "        \n",
        "        axs[i].imshow(image)\n",
        "        axs[i].set_xticks([])\n",
        "        axs[i].set_yticks([])\n",
        "        axs[i].set_title(titles[label], fontsize=12)\n",
        "    \n",
        "    # Hide any unused subplots\n",
        "    # for j in range(6, len(axs)):\n",
        "    #     axs[j].axis('off')\n",
        "    \n",
        "    fig.tight_layout()\n",
        "    return fig\n",
        "\n",
        "torch.manual_seed(88)\n",
        "first_images, first_labels = get_one_per_class(train_loader.dataset, 6)\n",
        "fig = figure2(first_images, first_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "F."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp9hamqkx7Pg"
      },
      "source": [
        "# Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8w4MPnpx7Pg"
      },
      "outputs": [],
      "source": [
        "class CNN2(nn.Module):\n",
        "    def __init__(self, n_feature, p=0.0):\n",
        "        super(CNN2, self).__init__()\n",
        "        self.n_feature = n_feature\n",
        "        self.p = p\n",
        "        # Creates the convolution layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=3,\n",
        "                               out_channels=n_feature,\n",
        "                               kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(in_channels=n_feature,\n",
        "                               out_channels=n_feature,\n",
        "                               kernel_size=3)\n",
        "        # Creates the linear layers\n",
        "        # Where do this 5 * 5 come from?! Check it below\n",
        "        self.fc1 = nn.Linear(n_feature * 5 * 5, 50)\n",
        "        self.fc2 = nn.Linear(50, 6) # here we set the 6 classes in total\n",
        "        # Creates dropout layers\n",
        "        self.drop = nn.Dropout(self.p)\n",
        "\n",
        "    def featurizer(self, x):\n",
        "        # Featurizer\n",
        "        # First convolutional block\n",
        "        # 3@28x28 -> n_feature@26x26 -> n_feature@13x13\n",
        "        x = self.conv1(x)\n",
        "        x = F.elu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        # Second convolutional block\n",
        "        # n_feature * @13x13 -> n_feature@11x11 -> n_feature@5x5\n",
        "        x = self.conv2(x)\n",
        "        x = F.elu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        # Input dimension (n_feature@5x5)\n",
        "        # Output dimension (n_feature * 5 * 5)\n",
        "        x = nn.Flatten()(x)\n",
        "        return x\n",
        "\n",
        "    def classifier(self, x):\n",
        "        # Classifier\n",
        "        # Hidden Layer\n",
        "        # Input dimension (n_feature * 5 * 5)\n",
        "        # Output dimension (50)\n",
        "        if self.p > 0:\n",
        "            x = self.drop(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        # Output Layer\n",
        "        # Input dimension (50)\n",
        "        # Output dimension (6)\n",
        "        if self.p > 0:\n",
        "            x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.featurizer(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU9Sv3_rW_Uz"
      },
      "source": [
        "# Case Study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjwBhjA1x7Pl"
      },
      "source": [
        "## Model Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- função de erro\n",
        "- AdamW\n",
        "- trocar a Relu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8AnsR9RGCaN"
      },
      "source": [
        "In this experiment the ```n_feature``` will be increased from 5 to 15."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(13)\n",
        "\n",
        "# Model/Architecture\n",
        "model_cnn2 = CNN2(n_feature=5, p=0.3)\n",
        "\n",
        "# Loss function\n",
        "multi_loss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "\n",
        "# Optimizer\n",
        "optimizer_cnn2 = optim.AdamW(model_cnn2.parameters(), lr=3e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCcNFhatLlF7",
        "outputId": "743c4e31-8eb6-4c26-f15d-d5ffb112b39d"
      },
      "outputs": [],
      "source": [
        "optimizer_cnn2.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_2-_imRx7Pl"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's keep 10 epochs due memory and computational power!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v23rWHLTx7Pl"
      },
      "outputs": [],
      "source": [
        "arch_cnn2 = Architecture(model_cnn2,\n",
        "                        multi_loss_fn,\n",
        "                        optimizer_cnn2)\n",
        "arch_cnn2.set_loaders(train_loader, val_loader)\n",
        "arch_cnn2.train(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz3dTBTkKfpJ",
        "outputId": "34b76787-d154-4e42-9587-06ce7eea66a7"
      },
      "outputs": [],
      "source": [
        "arch_cnn2.count_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "dDSyJu6Rx7Pm",
        "outputId": "18e93a52-9155-4ebe-8142-f9e7ebf1d5e7"
      },
      "outputs": [],
      "source": [
        "fig = arch_cnn2.plot_losses()\n",
        "\n",
        "fig.savefig('../images/model_1/cnn2_losses.png', dpi=300, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot0FT1aAx7Pm"
      },
      "source": [
        "### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRWhimcwx7Pm",
        "outputId": "7711df50-e798-4f1a-a188-75371c255e20"
      },
      "outputs": [],
      "source": [
        "Architecture.loader_apply(val_loader,\n",
        "                          arch_cnn2.correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN1LrRNuknlw",
        "outputId": "c28768a8-5a00-4630-a59f-35959d29386b"
      },
      "outputs": [],
      "source": [
        "Architecture.loader_apply(val_loader,\n",
        "                          arch_cnn2.correct).sum(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkiRae49lBQf",
        "outputId": "6df7ce0f-42fa-4c88-e73e-21cb19145668"
      },
      "outputs": [],
      "source": [
        "(lambda x: x[0].item() / x[1].item())(Architecture.loader_apply(val_loader,\n",
        "                                                                arch_cnn2.correct).sum(axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creates a Confusion Matrix\n",
        "def confusion_matrix(loader, arch):\n",
        "    # Gets the number of classes\n",
        "    num_classes = len(loader.dataset.classes)\n",
        "    # Initializes the confusion matrix\n",
        "    cm = torch.zeros(num_classes, num_classes, dtype=torch.int64)\n",
        "\n",
        "    # Loops through the mini-batches\n",
        "    for x_batch, y_batch in loader:\n",
        "        x_batch = x_batch.to(arch.device)\n",
        "        y_batch = y_batch.to(arch.device)\n",
        "\n",
        "        # Gets the predictions\n",
        "        yhat = arch.predict(x_batch)\n",
        "        # Converts logits to class indices\n",
        "        predicted = torch.argmax(torch.tensor(yhat), dim=1)\n",
        "\n",
        "        # Updates the confusion matrix\n",
        "        for t, p in zip(y_batch.view(-1), predicted.view(-1)):\n",
        "            cm[t.long(), p.long()] += 1\n",
        "\n",
        "    return cm\n",
        "\n",
        "# Computes the confusion matrix for the validation set\n",
        "cm = confusion_matrix(val_loader, arch_cnn2)\n",
        "\n",
        "# Ploting the confusion matrix\n",
        "def plot_confusion_matrix(cm, classes, title):\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    cax = ax.matshow(cm.numpy(), cmap=plt.cm.coolwarm_r)\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set axis labels\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('True')\n",
        "    ax.set_title(title)\n",
        "\n",
        "    # Set ticks and labels\n",
        "    ax.set_xticks(np.arange(len(classes)))\n",
        "    ax.set_yticks(np.arange(len(classes)))\n",
        "    ax.set_xticklabels(classes, rotation=90, fontsize=10)\n",
        "    ax.set_yticklabels(classes, fontsize=10)\n",
        "\n",
        "    # Annotate each cell with the numeric value\n",
        "    for i in range(len(classes)):\n",
        "        for j in range(len(classes)):\n",
        "            ax.text(j, i, cm[i, j].item(), ha='center', va='center', color='white' if cm[i, j] > cm.max() / 2 else 'black')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Plot the confusion matrix\n",
        "fig_cm = plot_confusion_matrix(cm, arch_cnn2.val_loader.dataset.classes,'Confusion Matrix - Model 1')\n",
        "\n",
        "# Save the cm image\n",
        "fig_cm.savefig('../images/model_1/C_matrix.png', dpi=300, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2ZUMxTjx7Pm"
      },
      "source": [
        "### Regularizing Effect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYzWhT5Ex7Pm"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(13)\n",
        "# Model Configuration\n",
        "model_cnn2_nodrop = CNN2(n_feature=15, p=0.0)\n",
        "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
        "optimizer_cnn2_nodrop = optim.Adam(model_cnn2_nodrop.parameters(), lr=3e-4)\n",
        "# Model Training\n",
        "arch_cnn2_nodrop = Architecture(model_cnn2_nodrop, multi_loss_fn, optimizer_cnn2_nodrop)\n",
        "arch_cnn2_nodrop.set_loaders(train_loader, val_loader)\n",
        "arch_cnn2_nodrop.train(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VM30IJOjK6e"
      },
      "outputs": [],
      "source": [
        "def figure11(losses, val_losses, losses_nodrop, val_losses_nodrop):\n",
        "    fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n",
        "    axs.plot(losses, 'g', label='Training Losses - Dropout')\n",
        "    axs.plot(val_losses, 'y', label='Validation Losses - Dropout')\n",
        "    axs.plot(losses_nodrop, 'g--', label='Training Losses - No Dropout')\n",
        "    axs.plot(val_losses_nodrop, 'y--', label='Validation Losses - No Dropout')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Regularizing Effect')\n",
        "    fig.legend(loc='upper right', bbox_to_anchor=(0.99, 0.935))\n",
        "    fig.tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "4bpE2wmLx7Pm",
        "outputId": "61aeb499-d654-4c07-b795-89283fdeb27d"
      },
      "outputs": [],
      "source": [
        "fig = figure11(arch_cnn2.losses, arch_cnn2.val_losses, arch_cnn2_nodrop.losses, arch_cnn2_nodrop.val_losses)\n",
        "\n",
        "# Save the figure\n",
        "fig.savefig('../images/model_1/regularizing_effect.png', dpi=300, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Acurracy without the dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQwHu31_x7Pm",
        "outputId": "f9708602-adaf-4d92-e52f-b0b39c9f11c2"
      },
      "outputs": [],
      "source": [
        "print('Correctly classified in train:',\n",
        "    Architecture.loader_apply(train_loader, arch_cnn2_nodrop.correct).sum(axis=0),\n",
        "    '\\n','Correctly classified in val:',\n",
        "    Architecture.loader_apply(val_loader, arch_cnn2_nodrop.correct).sum(axis=0)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ_qXhn7mCen",
        "outputId": "6c66122c-aaf7-4742-ab9b-063b1dade0d5"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy in train:\",\n",
        "    (lambda x: x[0].item() / x[1].item())(Architecture.loader_apply(train_loader, arch_cnn2_nodrop.correct).sum(axis=0)),\n",
        "    '\\n','Accuracy in val:',\n",
        "    (lambda x: x[0].item() / x[1].item())(Architecture.loader_apply(val_loader, arch_cnn2_nodrop.correct).sum(axis=0))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(val_loader, arch_cnn2_nodrop)\n",
        "\n",
        "# Ploting the confusion matrix\n",
        "def plot_confusion_matrix2(cm, classes, title):\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    cax = ax.matshow(cm.numpy(), cmap=plt.cm.berlin_r)\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set axis labels\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('True')\n",
        "    ax.set_title(title)\n",
        "\n",
        "    # Set ticks and labels\n",
        "    ax.set_xticks(np.arange(len(classes)))\n",
        "    ax.set_yticks(np.arange(len(classes)))\n",
        "    ax.set_xticklabels(classes, rotation=90, fontsize=10)\n",
        "    ax.set_yticklabels(classes, fontsize=10)\n",
        "\n",
        "    # Annotate each cell with the numeric value\n",
        "    for i in range(len(classes)):\n",
        "        for j in range(len(classes)):\n",
        "            ax.text(j, i, cm[i, j].item(), ha='center', va='center', color='white' if cm[i, j] > cm.max() / 2 else 'black')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "fig_cm = plot_confusion_matrix2(cm, arch_cnn2_nodrop.val_loader.dataset.classes,\"Confusion Matrix - No Dropout\")\n",
        "\n",
        "# Save the cm image\n",
        "\n",
        "fig_cm.savefig('../images/C_matrix_nodrop.png', dpi=300, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Architecture.loader_apply(val_loader,\n",
        "                          arch_cnn2_nodrop.correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Acurracy with Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4OmbK88x7Pm",
        "outputId": "f3dadd44-cdf2-41d6-bdf7-4c3a430686db"
      },
      "outputs": [],
      "source": [
        "print('Correctly classified in train:',\n",
        "    Architecture.loader_apply(train_loader, arch_cnn2.correct).sum(axis=0),\n",
        "    '\\n','Correctly classified in val:',\n",
        "    Architecture.loader_apply(val_loader, arch_cnn2.correct)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjCSH3bymW5w",
        "outputId": "0a9dcead-d591-4f6e-9a6e-d305f18c1373"
      },
      "outputs": [],
      "source": [
        "print('Accuracy in train:',\n",
        "    (lambda x: x[0].item() / x[1].item())(Architecture.loader_apply(train_loader, arch_cnn2.correct).sum(axis=0)),\n",
        "    '\\n','Accuracy in val:',\n",
        "    (lambda x: x[0].item() / x[1].item())(Architecture.loader_apply(val_loader, arch_cnn2.correct).sum(axis=0))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9tCbHGDx7Pm"
      },
      "source": [
        "### Visualizing Filters\n",
        "\n",
        "The conv1 filter visualization shows 3-channel (RGB) 3×3 kernels that learn low-level features such as edges and color gradients. These weights exhibit clear directional patterns and contrasts typical of early-layer feature detectors.\n",
        "\n",
        "In contrast, the conv2 visualization displays 5-channel 3×3 kernels, with each channel corresponding to a feature map output from conv1. These filters capture more abstract, higher-order combinations of the first-layer features. The weights appear less interpretable in isolation, reflecting their role in integrating and recombining simpler patterns into more complex representations useful for classification.\n",
        "\n",
        "This progression illustrates the hierarchical nature of CNNs: conv1 learns localized, low-level features, while conv2 composes them into richer, more discriminative abstractions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH6OlkjKx7Pm",
        "outputId": "7353db26-63e1-4c72-b55a-f6ed3d38a512"
      },
      "outputs": [],
      "source": [
        "model_cnn2.conv1.weight.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        },
        "id": "npjMerDKx7Pm",
        "outputId": "b90085a5-0b01-495c-84c6-0cc5ddca6107"
      },
      "outputs": [],
      "source": [
        "fig = arch_cnn2.visualize_filters('conv1')\n",
        "\n",
        "fig.savefig('../images/filters_conv1.png', dpi=300, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4wTDLadx7Pm",
        "outputId": "ea17559b-c43d-4875-bfa0-6905b9c28d5a"
      },
      "outputs": [],
      "source": [
        "model_cnn2.conv2.weight.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        },
        "id": "H6EPMkGix7Pn",
        "outputId": "1da15f23-64de-4f37-8b94-69aefe513f54"
      },
      "outputs": [],
      "source": [
        "fig = arch_cnn2.visualize_filters('conv2')\n",
        "\n",
        "fig.savefig('../images/filters_conv2.png', dpi=300, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving the model checkpoint\n",
        "\n",
        "arch_cnn2.save_checkpoint(\"../models/base_model.pth\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "cdo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
